---
Fecha de creaci贸n: 2025-08-31 17:53
Fecha de Modificaci贸n: 2025-08-31 17:53
tags: 
Tema:
---


##  Idea/Concepto 

La tokenizaci贸n es un proceso mediante el cual el lenguaje natural se descompone en partes m谩s peque帽as y significativas a las que se les llama tokens. Los tokens pueden ser palabras, subpalabras o caracteres, los cuales se pueden gestionar con algoritmos avanzados como el Byte Pair Encoding (BPE), estos algoritmos gestionan de manera eficiente el vocabulario y las palabras desconocidas. Adem谩s, dichos tokens son luego convertidos a IDs num茅ricos mediante un vocabulario, lo cual es importante para las redes neuronales, ya que estas operan exclusivamente con n煤meros. Cabe recalcar que el esquema de tokenizaci贸n elegido impacta directamente en el tama帽o efectivo de la ventana de contexto que un modelo puede procesar.
##  Puntos Claves (Opcional)
- 

##  Connections
- [[Redes Neuronales]]

##  Personal Insight (Opcional)
- 
## Ь Recursos (Opcional)
- 
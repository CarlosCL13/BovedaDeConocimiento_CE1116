---
Fecha de creación: 2025-08-31 17:53
Fecha de Modificación: 2025-08-31 17:53
tags: 
Tema:
---


## 📚 Idea/Concepto 

La tokenización es un proceso mediante el cual el lenguaje natural se descompone en partes más pequeñas y significativas a las que se les llama tokens. Los tokens pueden ser palabras, subpalabras o caracteres, los cuales se pueden gestionar con algoritmos avanzados como el Byte Pair Encoding (BPE), estos algoritmos gestionan de manera eficiente el vocabulario y las palabras desconocidas. Además, dichos tokens son luego convertidos a IDs numéricos mediante un vocabulario, lo cual es importante para las redes neuronales, ya que estas operan exclusivamente con números. Cabe recalcar que el esquema de tokenización elegido impacta directamente en el tamaño efectivo de la ventana de contexto que un modelo puede procesar.
## 📌 Puntos Claves (Opcional)
- 

## 🔗 Connections
- [[Redes Neuronales]]

## 💡 Personal Insight (Opcional)
- 
## 🧾 Recursos (Opcional)
- 